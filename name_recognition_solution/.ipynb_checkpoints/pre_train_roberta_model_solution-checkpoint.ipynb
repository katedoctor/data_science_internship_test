{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad9dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import RobertaTokenizerFast, RobertaForTokenClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5c9106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mount Everest is the highest mountain in the w...</td>\n",
       "      <td>Mount Everest</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The climbers faced harsh weather conditions on...</td>\n",
       "      <td>K2</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mt kilimanjaro hike difficulty level</td>\n",
       "      <td>Mount Kilimanjaro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mount Fuji sunrise viewing spots</td>\n",
       "      <td>Mount Fuji</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best viewpoints to see elbrus mountain</td>\n",
       "      <td>Mount Elbrus</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence             Entity  \\\n",
       "0  Mount Everest is the highest mountain in the w...      Mount Everest   \n",
       "1  The climbers faced harsh weather conditions on...                 K2   \n",
       "2               mt kilimanjaro hike difficulty level  Mount Kilimanjaro   \n",
       "3                   Mount Fuji sunrise viewing spots         Mount Fuji   \n",
       "4             best viewpoints to see elbrus mountain       Mount Elbrus   \n",
       "\n",
       "   Start   End  \n",
       "0    0.0  13.0  \n",
       "1   47.0  48.0  \n",
       "2    0.0  13.0  \n",
       "3    0.0   9.0  \n",
       "4   23.0  28.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file with the data collection\n",
    "\n",
    "file_path = \"mountain_sentance_collection.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.fillna({'Entity': 'NoEntity', 'Start': -1, 'End': -1}, inplace=True)\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dee4be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def label_entities(sentence, entity, start, end):\n",
    "    tokenize_sentence = sentence.lower().split()  # Tokenize the sentence (convert to lowercase for case-insensitivity)\n",
    "    entity = entity.lower().split()  # Tokenize the entity (convert to lowercase for case-insensitivity)\n",
    "    entity_label = [0] * len(tokenize_sentence)  # Initialize entity labels as 0 (for the begging)\n",
    "    \n",
    "    for i, token in  enumerate(tokenize_sentence):\n",
    "        start_idx = sentence.lower().find(token.lower())\n",
    "        end_idx = start_idx + len(token) - 1\n",
    "        \n",
    "        # if index of token between defined positions than it's mountain\n",
    "        if (start <= start_idx <= end and start <= end_idx <= end):\n",
    "            entity_label[i] = 1\n",
    "\n",
    "    return tokenize_sentence, entity_label\n",
    "\n",
    "train_data = [label_entities(row['Sentence'], row['Entity'], int(row['Start']), int(row['End'])) for _, row in train_df.iterrows()]\n",
    "val_data = [label_entities(row['Sentence'], row['Entity'], int(row['Start']), int(row['End'])) for _, row in val_df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "\n",
    "# Tokenize and pad sentences, and create attention masks\n",
    "def tokenize_and_pad(data):\n",
    "    sentences, labels = zip(*data)\n",
    "    tokenized_inputs = tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors=\"pt\", is_split_into_words=True)\n",
    "    max_seq_length = 512\n",
    "    \n",
    "    input_ids = tokenized_inputs[\"input_ids\"]\n",
    "    attention_masks = tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    \n",
    "    padded_labels = []\n",
    "    for label in labels:\n",
    "        if len(label) < max_seq_length:\n",
    "            label = label + [0] * (max_seq_length - len(label))\n",
    "        elif len(label) > max_seq_length:\n",
    "            label = label[:max_seq_length]\n",
    "        padded_labels.append(label)\n",
    "    \n",
    "    labels = torch.tensor(padded_labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "# Preparing the training and validation data\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_and_pad(train_data)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_and_pad(val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01544951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT MODEL\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\", # has better performance than BERT\n",
    "    num_labels = 2,  # 2 for binary classification\n",
    "    output_attentions = False,  \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74340b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Validation Loss: {avg_val_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "\n",
    "    # Flatten the predictions and true labels\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Convert logits to class predictions\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=2).flatten()\n",
    "    flat_true_labels = flat_true_labels.flatten()\n",
    "\n",
    "    return classification_report(flat_true_labels, flat_predictions, labels=[0,1], zero_division=1)\n",
    "\n",
    "# Evaluate on validation set\n",
    "report = evaluate_model(model, val_dataloader)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Convert logits to class predictions\n",
    "            batch_predictions = np.argmax(logits, axis=2).flatten()\n",
    "            batch_true_labels = label_ids.flatten()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            true_labels.extend(batch_true_labels)\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# Get predictions and true labels\n",
    "predictions, true_labels = get_predictions(model, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=[0, 1])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'roberta_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e4970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
